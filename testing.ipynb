{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "001bfcce-9376-4bf8-99bf-9896b6aa91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "class DominantColor:\n",
    "\n",
    "    resize_value: int = 16\n",
    "    minimum_percent_difference_of_rgb: int = 10\n",
    "\n",
    "    # def __init__(self, image_path: str) -> None:\n",
    "    #     self.image_path = image_path\n",
    "    #     self.image = Image.open(self.image_path)\n",
    "    #     self.dominant_color: str = \"\"\n",
    "    #     self.r: int = 0\n",
    "    #     self.g: int = 0\n",
    "    #     self.b: int = 0\n",
    "    #     self.l: int = 0\n",
    "    #     self.resized_image = self.image.resize(\n",
    "    #         (DominantColor.resize_value, DominantColor.resize_value), Image.ANTIALIAS\n",
    "    #     ).convert(\"RGBA\")\n",
    "    #     self.image.close()\n",
    "    #     self.image_data = self.resized_image.getdata()\n",
    "    #     self.generate_dominant_color_of_pixels_of_image_array()\n",
    "    #     self.resized_image.close()\n",
    "    #     self.counter = Counter(self.dominant_color_of_pixels_of_image_array)\n",
    "    #     self.set_rgbl_value_of_image()\n",
    "    #     self.set_dominat_color_of_image()\n",
    "    #     self.rgb = (self.r, self.g, self.b)\n",
    "    #     self.rgbl = (self.r, self.g, self.b, self.l)\n",
    "\n",
    "    def __init__(self, image_arr) -> None:\n",
    "        self.image = Image.fromarray(image_arr.astype('uint8'), 'RGB')\n",
    "        self.dominant_color: str = \"\"\n",
    "        self.r: int = 0\n",
    "        self.g: int = 0\n",
    "        self.b: int = 0\n",
    "        self.l: int = 0\n",
    "        self.resized_image = self.image.resize(\n",
    "            (DominantColor.resize_value, DominantColor.resize_value), Image.Resampling.LANCZOS\n",
    "        ).convert(\"RGBA\")\n",
    "        self.image.close()\n",
    "        self.image_data = self.resized_image.getdata()\n",
    "        self.generate_dominant_color_of_pixels_of_image_array()\n",
    "        self.resized_image.close()\n",
    "        self.counter = Counter(self.dominant_color_of_pixels_of_image_array)\n",
    "        self.set_rgbl_value_of_image()\n",
    "        self.set_dominat_color_of_image()\n",
    "        self.rgb = (self.r, self.g, self.b)\n",
    "        self.rgbl = (self.r, self.g, self.b, self.l)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            \"DominantColor(r:%s g:%s b:%s l:%s; dominant_color:%s; resize_value:%s; minimum_percent_difference_of_rgb:%s)\"\n",
    "            % (\n",
    "                self.r,\n",
    "                self.g,\n",
    "                self.b,\n",
    "                self.l,\n",
    "                self.dominant_color,\n",
    "                str(self.resize_value),\n",
    "                str(self.minimum_percent_difference_of_rgb),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.dominant_color\n",
    "\n",
    "    def set_dominat_color_of_image(self) -> None:\n",
    "        self.mpd = int(\n",
    "            self.total_pixels * (DominantColor.minimum_percent_difference_of_rgb / 100)\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            max(\n",
    "                set(self.dominant_color_of_pixels_of_image_array),\n",
    "                key=self.dominant_color_of_pixels_of_image_array.count,\n",
    "            )\n",
    "            == \"l\"\n",
    "        ):\n",
    "            self.dominant_color = \"l\"\n",
    "            return\n",
    "\n",
    "        if (self.r - self.mpd) > self.g and (self.r - self.mpd) > self.b:\n",
    "            self.dominant_color = \"r\"\n",
    "            return\n",
    "        if (self.g - self.mpd) > self.b and (self.g - self.mpd) > self.r:\n",
    "            self.dominant_color = \"g\"\n",
    "            return\n",
    "        if (self.b - self.mpd) > self.r and (self.b - self.mpd) > self.g:\n",
    "            self.dominant_color = \"b\"\n",
    "            return\n",
    "        self.dominant_color = \"n\"\n",
    "\n",
    "    def set_rgbl_value_of_image(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets the value for attribute r,g,b and l.\n",
    "\n",
    "        Note that these attributes indicates the number of\n",
    "        pixels which have dominating r,g,b and l values repectively.\n",
    "\n",
    "        The sum of r,g,b and l should be equal to total_pixels attribute.\n",
    "        \"\"\"\n",
    "        self.r = self.counter.get(\"r\", 0)\n",
    "        self.g = self.counter.get(\"g\", 0)\n",
    "        self.b = self.counter.get(\"b\", 0)\n",
    "        self.l = self.counter.get(\"l\", 0)\n",
    "\n",
    "    def generate_dominant_color_of_pixels_of_image_array(self) -> None:\n",
    "\n",
    "        self.total_pixels: int = 0\n",
    "        self.dominant_color_of_pixels_of_image_array: List = []\n",
    "\n",
    "        for i in range(DominantColor.resize_value):\n",
    "\n",
    "            for j in range(DominantColor.resize_value):\n",
    "\n",
    "                self.dominant_color_of_pixels_of_image_array.append(\n",
    "                    self.dominant_color_of_pixel(self.image_data.getpixel((i, j)))\n",
    "                )\n",
    "\n",
    "                self.total_pixels += 1\n",
    "\n",
    "    def dominant_color_of_pixel(self, pixel: Tuple[int, int, int, int]) -> str:\n",
    "\n",
    "        r, g, b = pixel[0], pixel[1], pixel[2]\n",
    "\n",
    "        if r > g and r > b:\n",
    "            return \"r\"\n",
    "\n",
    "        if g > b and g > r:\n",
    "            return \"g\"\n",
    "\n",
    "        if b > r and b > g:\n",
    "            return \"b\"\n",
    "\n",
    "        return \"l\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6252fb9-9f2b-4428-825e-cd4d6a8febf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 4 persons, 52.1ms\n",
      "Speed: 2.2ms preprocess, 52.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x192 1 long sleeve outwear, 1 long sleeve top, 1 trousers, 19.8ms\n",
      "Speed: 0.7ms preprocess, 19.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 192)\n",
      "0.45435002\n",
      "(0, 1, 252)\n",
      "0.4506395\n",
      "(0, 1, 255)\n",
      "0.35052428\n",
      "(0, 1, 252)\n",
      "\n",
      "0: 640x224 1 long sleeve top, 1 trousers, 20.8ms\n",
      "Speed: 0.9ms preprocess, 20.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 224)\n",
      "0.7872292\n",
      "(205, 27, 18)\n",
      "0.7156097\n",
      "(0, 0, 256)\n",
      "\n",
      "0: 640x256 1 long sleeve top, 1 short sleeve top, 1 trousers, 21.7ms\n",
      "Speed: 5.2ms preprocess, 21.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 256)\n",
      "0.8220574\n",
      "(0, 137, 102)\n",
      "0.40657592\n",
      "(0, 22, 226)\n",
      "0.38579988\n",
      "(0, 22, 226)\n",
      "\n",
      "0: 640x96 (no detections), 14.1ms\n",
      "Speed: 2.1ms preprocess, 14.1ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 96)\n",
      "(23, 81, 139)\n",
      "(58, 151, 34)\n",
      "['person #512 blue long sleeve top, blue trousers, blue long sleeve outwear, ', 'person #513 firebrick trousers, blue long sleeve top, ', 'person #514 teal trousers, mediumblue short sleeve top, mediumblue long sleeve top, ', 'person #515 darkslateblue unknown top, and forestgreen unknown bottom.'] [[     40.246      259.91      156.75      671.44]\n",
      " [     153.99      262.29      286.76       656.8]\n",
      " [     289.36      295.89      419.61      654.29]\n",
      " [     470.08      345.38         502      622.48]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import webcolors\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "detect_model = YOLO(\"yolo11s.pt\")\n",
    "clothes_model = YOLO(\"best.pt\")\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "def closest_color(requested_color):\n",
    "    min_distance = float('inf')\n",
    "    closest_name = None\n",
    "\n",
    "    # Iterate through all known colors in webcolors\n",
    "    for hex_code, color_name in webcolors._definitions._CSS3_HEX_TO_NAMES.items():\n",
    "        r, g, b = webcolors.hex_to_rgb(hex_code)\n",
    "        \n",
    "        # Calculate Euclidean distance\n",
    "        distance = ((r - requested_color[0]) ** 2 +\n",
    "                    (g - requested_color[1]) ** 2 +\n",
    "                    (b - requested_color[2]) ** 2) ** 0.5\n",
    "\n",
    "        # Update the closest name if this color is closer\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_name = color_name\n",
    "\n",
    "    return closest_name\n",
    "\n",
    "def classify_color(cropped_frame):\n",
    "    # Calculate the dominant color\n",
    "    dominant_color = DominantColor(cropped_frame)\n",
    "\n",
    "    print(dominant_color.rgb)\n",
    "    \n",
    "    # Find the closest named color\n",
    "    closest_name = closest_color(dominant_color.rgb)\n",
    "    \n",
    "    return closest_name\n",
    "\n",
    "# Example usage:\n",
    "# res_color = classify_color(cropped_frame)\n",
    "# print(\"Closest named color:\", res_color)\n",
    "    \n",
    "\n",
    "def mid_split_top_bottom(cropped_frame: np.ndarray):\n",
    "    # Get the height of the frame\n",
    "    height = cropped_frame.shape[0]\n",
    "    \n",
    "    # Calculate the midpoint\n",
    "    midpoint = height // 2\n",
    "    \n",
    "    # Split the frame into top and bottom halves\n",
    "    top_cropped_frame = cropped_frame[:midpoint, :]\n",
    "    bottom_cropped_frame = cropped_frame[midpoint:, :]\n",
    "\n",
    "    return f\"{classify_color(top_cropped_frame)} unknown top, and {classify_color(bottom_cropped_frame)} unknown bottom.\"\n",
    "    \n",
    "\n",
    "def detect_clothes(cropped_frame: np.ndarray) -> np.ndarray:\n",
    "    res = clothes_model(cropped_frame)[0]\n",
    "    detections = sv.Detections.from_ultralytics(res)\n",
    "    if len(detections.data['class_name']) >= 1:\n",
    "        clothes_str = \"\"\n",
    "        for xyxy, mask, confidence, class_id, tracker_id, data in detections:\n",
    "            print(confidence)\n",
    "            # Convert mask to uint8 if it is boolean\n",
    "            mask_uint8 = mask.astype(np.uint8) * 255\n",
    "\n",
    "            # Find contours of the solid mask annotation\n",
    "            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Get the bounding box of the largest contour\n",
    "                x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "                \n",
    "                # Center of the bounding box\n",
    "                center_x, center_y = x + w // 2, y + h // 2\n",
    "\n",
    "                # Reduce bounding box dimensions by half\n",
    "                reduced_side_length = max(w, h) // 2\n",
    "                half_reduced_side = reduced_side_length // 2\n",
    "\n",
    "                # Calculate the smaller square crop coordinates\n",
    "                square_x1 = max(center_x - half_reduced_side, 0)\n",
    "                square_y1 = max(center_y - half_reduced_side, 0)\n",
    "                square_x2 = min(center_x + half_reduced_side, cropped_frame.shape[1])\n",
    "                square_y2 = min(center_y + half_reduced_side, cropped_frame.shape[0])\n",
    "\n",
    "                # Crop the image to this smaller square region\n",
    "                cropped_img = cropped_frame[square_y1:square_y2, square_x1:square_x2]\n",
    "\n",
    "                with sv.ImageSink(target_dir_path=\"resu2lt\") as sink:\n",
    "                    sink.save_image(image=cropped_img)\n",
    "\n",
    "                \n",
    "            clothes_str += f\"{classify_color(cropped_img)} {data['class_name']}, \"\n",
    "\n",
    "            \n",
    "        return clothes_str\n",
    "    else:\n",
    "        return mid_split_top_bottom(cropped_frame)\n",
    "\n",
    "def detect_and_track_people(frame: np.ndarray) -> np.ndarray:\n",
    "    detect_res = detect_model(frame)[0]\n",
    "    detections = sv.Detections.from_ultralytics(detect_res)\n",
    "    detections = detections[detections.class_id == 0]\n",
    "    detections = detections[detections.confidence > 0.5]\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "\n",
    "    clothes = {}\n",
    "    #Crop the detected images and save\n",
    "    for xyxy, mask, confidence, class_id, tracker_id, data in detections:\n",
    "        #Segment each of the cropped image\n",
    "        cropped_img = sv.crop_image(image=frame, xyxy=xyxy)\n",
    "        \n",
    "        clothes[tracker_id] = detect_clothes(cropped_img)\n",
    "\n",
    "\n",
    "    labels = [\n",
    "        f\"{detect_res.names[class_id]} #{tracker_id} {clothes[tracker_id]}\"\n",
    "        for class_id, tracker_id\n",
    "        in zip(detections.class_id, detections.tracker_id)\n",
    "    ]\n",
    "\n",
    "    print(labels, detections.xyxy)\n",
    "\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        frame.copy(), detections=detections)\n",
    "    \n",
    "    return label_annotator.annotate(\n",
    "        annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "def pre_process_frame(frame: np.ndarray):\n",
    "    #Make it brighter\n",
    "    frame = cv2.convertScaleAbs(frame, alpha=1.2, beta=0)\n",
    "    return frame\n",
    "\n",
    "image = cv2.imread(\"test333.png\")\n",
    "image = pre_process_frame(image)\n",
    "result = detect_and_track_people(image)\n",
    "\n",
    "with sv.ImageSink(target_dir_path=\"result\") as sink:\n",
    "    # result = pre_process_frame(result)\n",
    "    sink.save_image(image=result)\n",
    "\n",
    "# video_info = sv.VideoInfo.from_video_path(\"test_3.mp4\")\n",
    "# frames_generator = sv.get_video_frames_generator(\"test_3.mp4\")\n",
    "\n",
    "# with sv.VideoSink(target_path=\"result_3.mp4\", video_info=video_info) as sink:\n",
    "#     for i, frame in enumerate(frames_generator):\n",
    "#         if i % 5 == 0:\n",
    "#             # frame = pre_process_frame(frame)\n",
    "#             frame = detect_and_track_people(frame)\n",
    "#             sink.write_frame(frame=frame)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831a3d3-f845-4728-b3f5-51063d1a7715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997c615-1599-444f-8b77-9eaf644ad4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
